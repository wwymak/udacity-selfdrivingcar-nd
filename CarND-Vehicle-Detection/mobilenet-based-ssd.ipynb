{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Lambda, Conv2D, MaxPooling2D, BatchNormalization, Dense, GlobalAveragePooling2D\n",
    "from keras.layers import  Flatten, Reshape, Concatenate, Activation, Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard, LambdaCallback\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras_ssd_loss import SSDLoss\n",
    "from keras_layer_AnchorBoxes import AnchorBoxes\n",
    "from keras_layer_L2Normalization import L2Normalization\n",
    "from ssd_box_encode_decode_utils import SSDBoxEncoder, decode_y, decode_y2\n",
    "from ssd_batch_generator import BatchGenerator\n",
    "\n",
    "from keras.applications.mobilenet import MobileNet, relu6, DepthwiseConv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.mobilenet import _depthwise_conv_block, _conv_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "# from moviepy.editor import VideoFileClip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "basemodel = MobileNet(include_top=False, weights='imagenet', input_shape = (128,128, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 128, 128, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 64, 64, 32)        864       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (Activation)      (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 64, 64, 32)        288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (Activation)  (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 64, 64, 64)        2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 64, 64, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (Activation)  (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 32, 32, 64)        576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (Activation)  (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 32, 32, 128)       8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (Activation)  (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 32, 32, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (Activation)  (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 32, 32, 128)       16384     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (Activation)  (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 16, 16, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (Activation)  (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 16, 16, 256)       32768     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (Activation)  (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 16, 16, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (Activation)  (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 16, 16, 256)       65536     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (Activation)  (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 8, 8, 256)         2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (Activation)  (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 8, 8, 512)         131072    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (Activation)  (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 8, 8, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (Activation)  (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 8, 8, 512)         262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (Activation)  (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 8, 8, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (Activation)  (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 8, 8, 512)         262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (Activation)  (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 8, 8, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (Activation)  (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 8, 8, 512)         262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (Activation)  (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 8, 8, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (Activation) (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 8, 8, 512)         262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (Activation) (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 8, 8, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (Activation) (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 8, 8, 512)         262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 8, 8, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (Activation) (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 4, 4, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (Activation) (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 4, 4, 1024)        524288    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 4, 4, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (Activation) (None, 4, 4, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 4, 4, 1024)        9216      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 4, 4, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (Activation) (None, 4, 4, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 4, 4, 1024)        1048576   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 4, 4, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (Activation) (None, 4, 4, 1024)        0         \n",
      "=================================================================\n",
      "Total params: 3,228,864\n",
      "Trainable params: 3,206,976\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "basemodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Container.get_layer of <keras.engine.training.Model object at 0x7f6b2d8861d0>>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basemodel.get_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ssd_mobilenet(image_shape, n_classes, l2_reg=0.0, scales = [0.08, 0.16, 0.32, 0.64, 0.96],\n",
    "                  variances = np.array([1.0, 1.0, 1.0, 1.0]),\n",
    "                aspect_ratios_per_layer=[[1.0, 2.0, 0.5],\n",
    "                                         [1.0, 2.0, 0.5],\n",
    "                                         [1.0, 2.0, 0.5],\n",
    "                                         [1.0, 2.0, 0.5]]):\n",
    "    n_predictor_layers = 4 # The number of predictor conv layers in the network\n",
    "    n_classes += 1 # Account for the background class.\n",
    "    # one box per aspect ratio, so for a predictor layer with 3 ratios then 3 boxes\n",
    "    n_boxes = []\n",
    "    scales = scales # for anchor boxes \n",
    "    variances = variances\n",
    "    steps = [None] * n_predictor_layers\n",
    "    offsets = [None] * n_predictor_layers\n",
    "    \n",
    "    for i in aspect_ratios_per_layer:\n",
    "        n_boxes.append(len(i))\n",
    "    \n",
    "    basemodel = MobileNet(include_top=False, weights='imagenet', input_shape = (128,128, 3))\n",
    "    \n",
    "    img_height, img_width, img_channels = image_shape[0], image_shape[1], image_shape[2]\n",
    "    x = Input(shape=image_shape)\n",
    "    x1 = Lambda(lambda x: K.tf.image.resize_images(x, (128, 128)))(x)\n",
    "    x1 = Lambda(lambda x: x - np.array(127.5), output_shape=(128, 128, 3),\n",
    "                   name='input_mean_norm')(x)\n",
    "    x1 = Lambda(lambda x: x/np.array(127.5), output_shape=(128, 128, 3),\n",
    "                   name='input_scaler')(x1)\n",
    "    \n",
    "    base = basemodel(x1)\n",
    "#     basemodel_compiled = Model(inputs=x, outputs=base)\n",
    "\n",
    "    #predictor layers for classes and boxes\n",
    "    classes7 = Conv2D(n_boxes[3] * n_classes, (3, 3), strides=(1, 1), padding=\"same\", \n",
    "                      kernel_initializer='glorot_normal', kernel_regularizer=l2(l2_reg), \n",
    "                      name='classes7')(base)\n",
    "    classes4 = Conv2D(n_boxes[0] * n_classes, (3, 3), strides=(1, 1), \n",
    "                      padding=\"same\", kernel_initializer='glorot_normal', \n",
    "                      kernel_regularizer=l2(l2_reg), name='classes4')(basemodel.get_layer(\"conv_pw_10_relu\").output)\n",
    "    classes5 = Conv2D(n_boxes[1] * n_classes, (3, 3), strides=(1, 1), \n",
    "                      padding=\"same\", kernel_initializer='glorot_normal', \n",
    "                      kernel_regularizer=l2(l2_reg), name='classes5')(basemodel.get_layer(\"conv_pw_11_relu\").output)\n",
    "    classes6 = Conv2D(n_boxes[2] * n_classes, (3, 3), strides=(1, 1), \n",
    "                      padding=\"same\", kernel_initializer='glorot_normal', \n",
    "                      kernel_regularizer=l2(l2_reg), name='classes6')(basemodel.get_layer(\"conv_pw_12_relu\").output)\n",
    "    \n",
    "    box7 = Conv2D(n_boxes[3] * 4, (3, 3), strides=(1, 1), padding=\"same\", \n",
    "                  kernel_initializer='glorot_normal', kernel_regularizer=l2(l2_reg), \n",
    "                  name='box7')(base)\n",
    "    # 4 coords per box -- xmin, xmax, ymun, ymax\n",
    "    box4 = Conv2D(n_boxes[0] * 4, (3, 3), strides=(1, 1), padding=\"same\", \n",
    "                  kernel_initializer='glorot_normal', kernel_regularizer=l2(l2_reg), \n",
    "                  name='box4')(basemodel.get_layer(\"conv_pw_10_relu\").output)\n",
    "    box5 = Conv2D(n_boxes[1] * 4, (3, 3), strides=(1, 1), padding=\"same\", \n",
    "                  kernel_initializer='glorot_normal', kernel_regularizer=l2(l2_reg), \n",
    "                  name='box5')(basemodel.get_layer(\"conv_pw_11_relu\").output)\n",
    "    box6 = Conv2D(n_boxes[2] * 4, (3, 3), strides=(1, 1), padding=\"same\", \n",
    "                  kernel_initializer='glorot_normal', kernel_regularizer=l2(l2_reg), \n",
    "                  name='box6')(basemodel.get_layer(\"conv_pw_12_relu\").output)\n",
    "    \n",
    "    \n",
    "    #use the custom anchorbox layer to generate predictions for boxes\n",
    "    anchors4 = AnchorBoxes(img_height, img_width, this_scale=scales[0], next_scale=scales[1], aspect_ratios=aspect_ratios_per_layer[0],\n",
    "                           two_boxes_for_ar1=False, this_steps=steps[0], this_offsets=offsets[0],\n",
    "                           limit_boxes=False, variances=variances, coords='centroids', normalize_coords=False, name='anchors4')(box4)\n",
    "    anchors5 = AnchorBoxes(img_height, img_width, this_scale=scales[1], next_scale=scales[2], aspect_ratios=aspect_ratios_per_layer[1],\n",
    "                           two_boxes_for_ar1=False, this_steps=steps[1], this_offsets=offsets[1],\n",
    "                           limit_boxes=False, variances=variances, coords='centroids', normalize_coords=False, name='anchors5')(box5)\n",
    "    anchors6 = AnchorBoxes(img_height, img_width, this_scale=scales[2], next_scale=scales[3], aspect_ratios=aspect_ratios_per_layer[2],\n",
    "                           two_boxes_for_ar1=False, this_steps=steps[2], this_offsets=offsets[2],\n",
    "                           limit_boxes=False, variances=variances, coords='centroids', normalize_coords=False, name='anchors6')(box6)\n",
    "    anchors7 = AnchorBoxes(img_height, img_width, this_scale=scales[3], next_scale=scales[4], aspect_ratios=aspect_ratios_per_layer[3],\n",
    "                           two_boxes_for_ar1=False, this_steps=steps[3], this_offsets=offsets[3],\n",
    "                           limit_boxes=False, variances=variances, coords='centroids', normalize_coords=False, name='anchors7')(box7)\n",
    "    \n",
    "    \n",
    "    # Reshape the class predictors tensor into (batch, height * width * n_boxes, n_classes) => the class is now the last variable for predcition\n",
    "    classes4_reshaped = Reshape((-1, n_classes), name='classes4_reshape')(classes4)\n",
    "    classes5_reshaped = Reshape((-1, n_classes), name='classes5_reshape')(classes5)\n",
    "    classes6_reshaped = Reshape((-1, n_classes), name='classes6_reshape')(classes6)\n",
    "    classes7_reshaped = Reshape((-1, n_classes), name='classes7_reshape')(classes7)\n",
    "    \n",
    "    # Reshape the box coords predictions, into tensors of shape (batch, height * width * n_boxes, 4) => box positions last for loss\n",
    "    # We want the four box coordinates isolated in the last axis to compute the smooth L1 loss\n",
    "    boxes4_reshaped = Reshape((-1, 4), name='boxes4_reshape')(box4)\n",
    "    boxes5_reshaped = Reshape((-1, 4), name='boxes5_reshape')(box5)\n",
    "    boxes6_reshaped = Reshape((-1, 4), name='boxes6_reshape')(box6)\n",
    "    boxes7_reshaped = Reshape((-1, 4), name='boxes7_reshape')(box7)\n",
    "    # Reshape the anchor box tensors, into tensors of shape (batch, height * width * n_boxes, 8)\n",
    "    anchors4_reshaped = Reshape((-1, 8), name='anchors4_reshape')(anchors4)\n",
    "    anchors5_reshaped = Reshape((-1, 8), name='anchors5_reshape')(anchors5)\n",
    "    anchors6_reshaped = Reshape((-1, 8), name='anchors6_reshape')(anchors6)\n",
    "    anchors7_reshaped = Reshape((-1, 8), name='anchors7_reshape')(anchors7)\n",
    "    #concat along the middle axis, since want to keep the batch and n_classes/8 untouched\n",
    "    class_concat = Concatenate(axis=1, name='concatenate_classes')([classes4_reshaped, \n",
    "                                classes5_reshaped,classes6_reshaped, \n",
    "                                classes7_reshaped])\n",
    "    boxes_concat = Concatenate(axis=1, name='concatenate_boxes')([boxes4_reshaped,\n",
    "                                                             boxes5_reshaped,\n",
    "                                                             boxes6_reshaped,\n",
    "                                                             boxes7_reshaped])\n",
    "\n",
    "    # Output shape of `anchors_final`: (batch, n_boxes_total, 8)\n",
    "    anchors_concat = Concatenate(axis=1, name='concatenate_anchors')([anchors4_reshaped,\n",
    "                                                                 anchors5_reshaped,\n",
    "                                                                 anchors6_reshaped,\n",
    "                                                                 anchors7_reshaped])\n",
    "    \n",
    "    classification_softmax = Activation('softmax', name='classes_softamx')(class_concat)\n",
    "    \n",
    "    prediction = Concatenate(axis=2,name='concatenate_output')([classification_softmax,\n",
    "                                                              boxes_concat,\n",
    "                                                              anchors_concat ])\n",
    "    \n",
    "    model = Model(inputs=x, outputs=prediction)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = BatchGenerator(box_output_format=['class_id', 'xmin', 'ymin', 'xmax', 'ymax'])\n",
    "val_dataset = BatchGenerator(box_output_format=['class_id', 'xmin', 'ymin', 'xmax', 'ymax'])\n",
    "\n",
    "# Training dataset\n",
    "train_images_dir      = 'udacity_driving_datasets/'\n",
    "train_labels_filename = 'udacity_driving_datasets/train_labels.csv'\n",
    "\n",
    "# Validation dataset\n",
    "val_images_dir      = 'udacity_driving_datasets/'\n",
    "val_labels_filename = 'udacity_driving_datasets/val_labels.csv'\n",
    "\n",
    "train_dataset.parse_csv(images_dir=train_images_dir,\n",
    "                        labels_filename=train_labels_filename,\n",
    "                        input_format=['image_name', 'xmin', 'xmax', 'ymin', 'ymax', 'class_id'], # This is the order of the first six columns in the CSV file that contains the labels for your dataset. If your labels are in XML format, maybe the XML parser will be helpful, check the documentation.\n",
    "                        include_classes='all')\n",
    "\n",
    "val_dataset.parse_csv(images_dir=val_images_dir,\n",
    "                      labels_filename=val_labels_filename,\n",
    "                      input_format=['image_name', 'xmin', 'xmax', 'ymin', 'ymax', 'class_id'],\n",
    "                      include_classes='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Graph disconnected: cannot obtain value for tensor Tensor(\"input_1:0\", shape=(?, 128, 128, 3), dtype=float32) at layer \"input_1\". The following previous layers were accessed without issue: []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-b53ec964d6b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                                          \u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                          \u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                                          [1.0, 2.0, 0.5]])\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# 3: Instantiate an Adam optimizer and the SSD loss function and compile the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-413ec1cea590>\u001b[0m in \u001b[0;36mssd_mobilenet\u001b[0;34m(image_shape, n_classes, l2_reg, scales, variances, aspect_ratios_per_layer)\u001b[0m\n\u001b[1;32m    112\u001b[0m                                                               anchors_concat ])\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[1;32m   1791\u001b[0m                                 \u001b[0;34m'The following previous layers '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m                                 \u001b[0;34m'were accessed without issue: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1793\u001b[0;31m                                 str(layers_with_complete_input))\n\u001b[0m\u001b[1;32m   1794\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1795\u001b[0m                         \u001b[0mcomputable_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Graph disconnected: cannot obtain value for tensor Tensor(\"input_1:0\", shape=(?, 128, 128, 3), dtype=float32) at layer \"input_1\". The following previous layers were accessed without issue: []"
     ]
    }
   ],
   "source": [
    "K.clear_session() # Clear previous models from memory.\n",
    "\n",
    "model = ssd_mobilenet((300, 480, 3), 5, l2_reg=0.0, \n",
    "                aspect_ratios_per_layer=[[1.0, 2.0, 0.5],\n",
    "                                         [1.0, 2.0, 0.5],\n",
    "                                         [1.0, 2.0, 0.5],\n",
    "                                         [1.0, 2.0, 0.5]])\n",
    "\n",
    "# 3: Instantiate an Adam optimizer and the SSD loss function and compile the model\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=5e-04)\n",
    "\n",
    "# with negative hard mining as per paper\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, n_neg_min=0, alpha=1.0)\n",
    "\n",
    "model.compile(optimizer=adam, loss=ssd_loss.compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'conv_pw_13_1/convolution:0' shape=(?, 4, 4, 1024) dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basemodel.get_layer('conv_pw_13').output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "x = Input(shape=(300, 480,3))\n",
    "x1 = Lambda(lambda x: K.tf.image.resize_images(x, (128, 128)))(x)\n",
    "x1 = Lambda(lambda x: x - np.array(127.5), output_shape=(128, 128, 3),\n",
    "               name='input_mean_norm')(x)\n",
    "x1 = Lambda(lambda x: x/np.array(127.5), output_shape=(128, 128, 3),\n",
    "               name='input_scaler')(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "basemodel = MobileNet(include_top=False, weights='imagenet', input_shape = (128,128, 3))\n",
    "base = basemodel(x1)\n",
    "\n",
    "\n",
    "    #predictor layers for classes and boxes\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_boxes = [4,4,4,4]\n",
    "n_classes = 5\n",
    "l2_reg=0.0\n",
    "classes7 = Conv2D(n_boxes[3] * n_classes, (3, 3), strides=(1, 1), padding=\"same\", \n",
    "                      kernel_initializer='glorot_normal', kernel_regularizer=l2(l2_reg), \n",
    "                      name='classes7')(base)\n",
    "classes4 = Conv2D(n_boxes[0] * n_classes, (3, 3), strides=(1, 1), \n",
    "                  padding=\"same\", kernel_initializer='glorot_normal', \n",
    "                  kernel_regularizer=l2(l2_reg), name='classes4')(basemodel.get_layer(\"conv_pw_10_relu\").output)\n",
    "classes5 = Conv2D(n_boxes[1] * n_classes, (3, 3), strides=(1, 1), \n",
    "                  padding=\"same\", kernel_initializer='glorot_normal', \n",
    "                  kernel_regularizer=l2(l2_reg), name='classes5')(basemodel.get_layer(\"conv_pw_11_relu\").output)\n",
    "classes6 = Conv2D(n_boxes[2] * n_classes, (3, 3), strides=(1, 1), \n",
    "                  padding=\"same\", kernel_initializer='glorot_normal', \n",
    "                  kernel_regularizer=l2(l2_reg), name='classes6')(basemodel.get_layer(\"conv_pw_12_relu\").output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=x, outputs=prediction)\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=5e-04)\n",
    "\n",
    "# with negative hard mining as per paper\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, n_neg_min=0, alpha=1.0)\n",
    "\n",
    "model.compile(optimizer=adam, loss=ssd_loss.compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 300, 480, 3)       0         \n",
      "_________________________________________________________________\n",
      "input_mean_norm (Lambda)     (None, 128, 128, 3)       0         \n",
      "_________________________________________________________________\n",
      "input_scaler (Lambda)        (None, 128, 128, 3)       0         \n",
      "_________________________________________________________________\n",
      "mobilenet_1.00_128 (Model)   (None, 4, 4, 1024)        3228864   \n",
      "=================================================================\n",
      "Total params: 3,228,864\n",
      "Trainable params: 3,206,976\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# classes4 = Conv2D(n_boxes[0] * n_classes, (3, 3), strides=(1, 1), \n",
    "#                       padding=\"same\", kernel_initializer='glorot_normal', \n",
    "#                       kernel_regularizer=l2(l2_reg), name='classes4')(basemodel_compiled.get_layer(\"conv_pw_10_relu\").output)\n",
    "basemodel_compiled.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MobileNet(input_shape=None,\n",
    "              alpha=1.0,\n",
    "              depth_multiplier=1,\n",
    "              dropout=1e-3,\n",
    "              include_top=True,\n",
    "              weights='imagenet',\n",
    "              input_tensor=None,\n",
    "              pooling=None,\n",
    "              classes=1000):\n",
    "    \"\"\"Instantiates the MobileNet architecture.\n",
    "    Note that only TensorFlow is supported for now,\n",
    "    therefore it only works with the data format\n",
    "    `image_data_format='channels_last'` in your Keras config\n",
    "    at `~/.keras/keras.json`.\n",
    "    To load a MobileNet model via `load_model`, import the custom\n",
    "    objects `relu6` and `DepthwiseConv2D` and pass them to the\n",
    "    `custom_objects` parameter.\n",
    "    E.g.\n",
    "    model = load_model('mobilenet.h5', custom_objects={\n",
    "                       'relu6': mobilenet.relu6,\n",
    "                       'DepthwiseConv2D': mobilenet.DepthwiseConv2D})\n",
    "    # Arguments\n",
    "        input_shape: optional shape tuple, only to be specified\n",
    "            if `include_top` is False (otherwise the input shape\n",
    "            has to be `(224, 224, 3)` (with `channels_last` data format)\n",
    "            or (3, 224, 224) (with `channels_first` data format).\n",
    "            It should have exactly 3 inputs channels,\n",
    "            and width and height should be no smaller than 32.\n",
    "            E.g. `(200, 200, 3)` would be one valid value.\n",
    "        alpha: controls the width of the network.\n",
    "            - If `alpha` < 1.0, proportionally decreases the number\n",
    "                of filters in each layer.\n",
    "            - If `alpha` > 1.0, proportionally increases the number\n",
    "                of filters in each layer.\n",
    "            - If `alpha` = 1, default number of filters from the paper\n",
    "                 are used at each layer.\n",
    "        depth_multiplier: depth multiplier for depthwise convolution\n",
    "            (also called the resolution multiplier)\n",
    "        dropout: dropout rate\n",
    "        include_top: whether to include the fully-connected\n",
    "            layer at the top of the network.\n",
    "        weights: one of `None` (random initialization),\n",
    "              'imagenet' (pre-training on ImageNet),\n",
    "              or the path to the weights file to be loaded.\n",
    "        input_tensor: optional Keras tensor (i.e. output of\n",
    "            `layers.Input()`)\n",
    "            to use as image input for the model.\n",
    "        pooling: Optional pooling mode for feature extraction\n",
    "            when `include_top` is `False`.\n",
    "            - `None` means that the output of the model\n",
    "                will be the 4D tensor output of the\n",
    "                last convolutional layer.\n",
    "            - `avg` means that global average pooling\n",
    "                will be applied to the output of the\n",
    "                last convolutional layer, and thus\n",
    "                the output of the model will be a\n",
    "                2D tensor.\n",
    "            - `max` means that global max pooling will\n",
    "                be applied.\n",
    "        classes: optional number of classes to classify images\n",
    "            into, only to be specified if `include_top` is True, and\n",
    "            if no `weights` argument is specified.\n",
    "    # Returns\n",
    "        A Keras model instance.\n",
    "    # Raises\n",
    "        ValueError: in case of invalid argument for `weights`,\n",
    "            or invalid input shape.\n",
    "        RuntimeError: If attempting to run this model with a\n",
    "            backend that does not support separable convolutions.\n",
    "    \"\"\"\n",
    "    if not (weights in {'imagenet', None} or os.path.exists(weights)):\n",
    "        raise ValueError('The `weights` argument should be either '\n",
    "                         '`None` (random initialization), `imagenet` '\n",
    "                         '(pre-training on ImageNet), '\n",
    "                         'or the path to the weights file to be loaded.')\n",
    "\n",
    "    if weights == 'imagenet' and include_top and classes != 1000:\n",
    "        raise ValueError('If using `weights` as ImageNet with `include_top` '\n",
    "                         'as true, `classes` should be 1000')\n",
    "\n",
    "    # Determine proper input shape and default size.\n",
    "    if input_shape is None:\n",
    "        default_size = 224\n",
    "    else:\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            rows = input_shape[1]\n",
    "            cols = input_shape[2]\n",
    "        else:\n",
    "            rows = input_shape[0]\n",
    "            cols = input_shape[1]\n",
    "\n",
    "        if rows == cols and rows in [128, 160, 192, 224]:\n",
    "            default_size = rows\n",
    "        else:\n",
    "            default_size = 224\n",
    "\n",
    "    input_shape = _obtain_input_shape(input_shape,\n",
    "                                      default_size=default_size,\n",
    "                                      min_size=32,\n",
    "                                      data_format=K.image_data_format(),\n",
    "                                      require_flatten=include_top,\n",
    "                                      weights=weights)\n",
    "\n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        row_axis, col_axis = (0, 1)\n",
    "    else:\n",
    "        row_axis, col_axis = (1, 2)\n",
    "    rows = input_shape[row_axis]\n",
    "    cols = input_shape[col_axis]\n",
    "\n",
    "    if weights == 'imagenet':\n",
    "        if depth_multiplier != 1:\n",
    "            raise ValueError('If imagenet weights are being loaded, '\n",
    "                             'depth multiplier must be 1')\n",
    "\n",
    "        if alpha not in [0.25, 0.50, 0.75, 1.0]:\n",
    "            raise ValueError('If imagenet weights are being loaded, '\n",
    "                             'alpha can be one of'\n",
    "                             '`0.25`, `0.50`, `0.75` or `1.0` only.')\n",
    "\n",
    "        if rows != cols or rows not in [128, 160, 192, 224]:\n",
    "            raise ValueError('If imagenet weights are being loaded, '\n",
    "                             'input must have a static square shape (one of '\n",
    "                             '(128,128), (160,160), (192,192), or (224, 224)).'\n",
    "                             ' Input shape provided = %s' % (input_shape,))\n",
    "\n",
    "    if K.image_data_format() != 'channels_last':\n",
    "        warnings.warn('The MobileNet family of models is only available '\n",
    "                      'for the input data format \"channels_last\" '\n",
    "                      '(width, height, channels). '\n",
    "                      'However your settings specify the default '\n",
    "                      'data format \"channels_first\" (channels, width, height).'\n",
    "                      ' You should set `image_data_format=\"channels_last\"` '\n",
    "                      'in your Keras config located at ~/.keras/keras.json. '\n",
    "                      'The model being returned right now will expect inputs '\n",
    "                      'to follow the \"channels_last\" data format.')\n",
    "        K.set_image_data_format('channels_last')\n",
    "        old_data_format = 'channels_first'\n",
    "    else:\n",
    "        old_data_format = None\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "\n",
    "    x = _conv_block(img_input, 32, alpha, strides=(2, 2))\n",
    "    x = _depthwise_conv_block(x, 64, alpha, depth_multiplier, block_id=1)\n",
    "\n",
    "    x = _depthwise_conv_block(x, 128, alpha, depth_multiplier,\n",
    "                              strides=(2, 2), block_id=2)\n",
    "    x = _depthwise_conv_block(x, 128, alpha, depth_multiplier, block_id=3)\n",
    "\n",
    "    x = _depthwise_conv_block(x, 256, alpha, depth_multiplier,\n",
    "                              strides=(2, 2), block_id=4)\n",
    "    x = _depthwise_conv_block(x, 256, alpha, depth_multiplier, block_id=5)\n",
    "\n",
    "    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier,\n",
    "                              strides=(2, 2), block_id=6)\n",
    "    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, block_id=7)\n",
    "    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, block_id=8)\n",
    "    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, block_id=9)\n",
    "    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, block_id=10)\n",
    "    x = _depthwise_conv_block(x, 512, alpha, depth_multiplier, block_id=11)\n",
    "\n",
    "    x = _depthwise_conv_block(x, 1024, alpha, depth_multiplier,\n",
    "                              strides=(2, 2), block_id=12)\n",
    "    x = _depthwise_conv_block(x, 1024, alpha, depth_multiplier, block_id=13)\n",
    "\n",
    "    if include_top:\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            shape = (int(1024 * alpha), 1, 1)\n",
    "        else:\n",
    "            shape = (1, 1, int(1024 * alpha))\n",
    "\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Reshape(shape, name='reshape_1')(x)\n",
    "        x = Dropout(dropout, name='dropout')(x)\n",
    "        x = Conv2D(classes, (1, 1),\n",
    "                   padding='same', name='conv_preds')(x)\n",
    "        x = Activation('softmax', name='act_softmax')(x)\n",
    "        x = Reshape((classes,), name='reshape_2')(x)\n",
    "    else:\n",
    "        if pooling == 'avg':\n",
    "            x = GlobalAveragePooling2D()(x)\n",
    "        elif pooling == 'max':\n",
    "            x = GlobalMaxPooling2D()(x)\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "\n",
    "    # Create model.\n",
    "    model = Model(inputs, x, name='mobilenet_%0.2f_%s' % (alpha, rows))\n",
    "\n",
    "    # load weights\n",
    "    if weights == 'imagenet':\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            raise ValueError('Weights for \"channels_last\" format '\n",
    "                             'are not available.')\n",
    "        if alpha == 1.0:\n",
    "            alpha_text = '1_0'\n",
    "        elif alpha == 0.75:\n",
    "            alpha_text = '7_5'\n",
    "        elif alpha == 0.50:\n",
    "            alpha_text = '5_0'\n",
    "        else:\n",
    "            alpha_text = '2_5'\n",
    "\n",
    "        if include_top:\n",
    "            model_name = 'mobilenet_%s_%d_tf.h5' % (alpha_text, rows)\n",
    "            weigh_path = BASE_WEIGHT_PATH + model_name\n",
    "            weights_path = get_file(model_name,\n",
    "                                    weigh_path,\n",
    "                                    cache_subdir='models')\n",
    "        else:\n",
    "            model_name = 'mobilenet_%s_%d_tf_no_top.h5' % (alpha_text, rows)\n",
    "            weigh_path = BASE_WEIGHT_PATH + model_name\n",
    "            weights_path = get_file(model_name,\n",
    "                                    weigh_path,\n",
    "                                    cache_subdir='models')\n",
    "        model.load_weights(weights_path)\n",
    "    elif weights is not None:\n",
    "        model.load_weights(weights)\n",
    "\n",
    "    if old_data_format:\n",
    "        K.set_image_data_format(old_data_format)\n",
    "    return model\n",
    "\n",
    "\n",
    "def _conv_block(inputs, filters, alpha, kernel=(3, 3), strides=(1, 1)):\n",
    "    \"\"\"Adds an initial convolution layer (with batch normalization and relu6).\n",
    "    # Arguments\n",
    "        inputs: Input tensor of shape `(rows, cols, 3)`\n",
    "            (with `channels_last` data format) or\n",
    "            (3, rows, cols) (with `channels_first` data format).\n",
    "            It should have exactly 3 inputs channels,\n",
    "            and width and height should be no smaller than 32.\n",
    "            E.g. `(224, 224, 3)` would be one valid value.\n",
    "        filters: Integer, the dimensionality of the output space\n",
    "            (i.e. the number of output filters in the convolution).\n",
    "        alpha: controls the width of the network.\n",
    "            - If `alpha` < 1.0, proportionally decreases the number\n",
    "                of filters in each layer.\n",
    "            - If `alpha` > 1.0, proportionally increases the number\n",
    "                of filters in each layer.\n",
    "            - If `alpha` = 1, default number of filters from the paper\n",
    "                 are used at each layer.\n",
    "        kernel: An integer or tuple/list of 2 integers, specifying the\n",
    "            width and height of the 2D convolution window.\n",
    "            Can be a single integer to specify the same value for\n",
    "            all spatial dimensions.\n",
    "        strides: An integer or tuple/list of 2 integers,\n",
    "            specifying the strides of the convolution along the width and height.\n",
    "            Can be a single integer to specify the same value for\n",
    "            all spatial dimensions.\n",
    "            Specifying any stride value != 1 is incompatible with specifying\n",
    "            any `dilation_rate` value != 1.\n",
    "    # Input shape\n",
    "        4D tensor with shape:\n",
    "        `(samples, channels, rows, cols)` if data_format='channels_first'\n",
    "        or 4D tensor with shape:\n",
    "        `(samples, rows, cols, channels)` if data_format='channels_last'.\n",
    "    # Output shape\n",
    "        4D tensor with shape:\n",
    "        `(samples, filters, new_rows, new_cols)` if data_format='channels_first'\n",
    "        or 4D tensor with shape:\n",
    "        `(samples, new_rows, new_cols, filters)` if data_format='channels_last'.\n",
    "        `rows` and `cols` values might have changed due to stride.\n",
    "    # Returns\n",
    "        Output tensor of block.\n",
    "    \"\"\"\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "    filters = int(filters * alpha)\n",
    "    x = Conv2D(filters, kernel,\n",
    "               padding='same',\n",
    "               use_bias=False,\n",
    "               strides=strides,\n",
    "               name='conv1')(inputs)\n",
    "    x = BatchNormalization(axis=channel_axis, name='conv1_bn')(x)\n",
    "    return Activation(relu6, name='conv1_relu')(x)\n",
    "\n",
    "\n",
    "def _depthwise_conv_block(inputs, pointwise_conv_filters, alpha,\n",
    "                          depth_multiplier=1, strides=(1, 1), block_id=1):\n",
    "    \"\"\"Adds a depthwise convolution block.\n",
    "    A depthwise convolution block consists of a depthwise conv,\n",
    "    batch normalization, relu6, pointwise convolution,\n",
    "    batch normalization and relu6 activation.\n",
    "    # Arguments\n",
    "        inputs: Input tensor of shape `(rows, cols, channels)`\n",
    "            (with `channels_last` data format) or\n",
    "            (channels, rows, cols) (with `channels_first` data format).\n",
    "        pointwise_conv_filters: Integer, the dimensionality of the output space\n",
    "            (i.e. the number of output filters in the pointwise convolution).\n",
    "        alpha: controls the width of the network.\n",
    "            - If `alpha` < 1.0, proportionally decreases the number\n",
    "                of filters in each layer.\n",
    "            - If `alpha` > 1.0, proportionally increases the number\n",
    "                of filters in each layer.\n",
    "            - If `alpha` = 1, default number of filters from the paper\n",
    "                 are used at each layer.\n",
    "        depth_multiplier: The number of depthwise convolution output channels\n",
    "            for each input channel.\n",
    "            The total number of depthwise convolution output\n",
    "            channels will be equal to `filters_in * depth_multiplier`.\n",
    "        strides: An integer or tuple/list of 2 integers,\n",
    "            specifying the strides of the convolution along the width and height.\n",
    "            Can be a single integer to specify the same value for\n",
    "            all spatial dimensions.\n",
    "            Specifying any stride value != 1 is incompatible with specifying\n",
    "            any `dilation_rate` value != 1.\n",
    "        block_id: Integer, a unique identification designating the block number.\n",
    "    # Input shape\n",
    "        4D tensor with shape:\n",
    "        `(batch, channels, rows, cols)` if data_format='channels_first'\n",
    "        or 4D tensor with shape:\n",
    "        `(batch, rows, cols, channels)` if data_format='channels_last'.\n",
    "    # Output shape\n",
    "        4D tensor with shape:\n",
    "        `(batch, filters, new_rows, new_cols)` if data_format='channels_first'\n",
    "        or 4D tensor with shape:\n",
    "        `(batch, new_rows, new_cols, filters)` if data_format='channels_last'.\n",
    "        `rows` and `cols` values might have changed due to stride.\n",
    "    # Returns\n",
    "        Output tensor of block.\n",
    "    \"\"\"\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "    pointwise_conv_filters = int(pointwise_conv_filters * alpha)\n",
    "\n",
    "    x = DepthwiseConv2D((3, 3),\n",
    "                        padding='same',\n",
    "                        depth_multiplier=depth_multiplier,\n",
    "                        strides=strides,\n",
    "                        use_bias=False,\n",
    "                        name='conv_dw_%d' % block_id)(inputs)\n",
    "    x = BatchNormalization(axis=channel_axis, name='conv_dw_%d_bn' % block_id)(x)\n",
    "    x = Activation(relu6, name='conv_dw_%d_relu' % block_id)(x)\n",
    "\n",
    "    x = Conv2D(pointwise_conv_filters, (1, 1),\n",
    "               padding='same',\n",
    "               use_bias=False,\n",
    "               strides=(1, 1),\n",
    "               name='conv_pw_%d' % block_id)(x)\n",
    "    x = BatchNormalization(axis=channel_axis, name='conv_pw_%d_bn' % block_id)(x)\n",
    "    return Activation(relu6, name='conv_pw_%d_relu' % block_id)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mobilenet_model(image_shape, n_classes, input_shape=(128,128,3), l2_reg=0.0, \n",
    "                    scales = [0.08, 0.16, 0.32, 0.64, 0.96],\n",
    "                  variances = np.array([1.0, 1.0, 1.0, 1.0]),\n",
    "                aspect_ratios_per_layer=[[1.0, 2.0, 0.5],\n",
    "                                         [1.0, 2.0, 0.5],\n",
    "                                         [1.0, 2.0, 0.5],\n",
    "                                         [1.0, 2.0, 0.5]]):\n",
    "    n_predictor_layers = 4 # The number of predictor conv layers in the network\n",
    "    n_classes += 1 # Account for the background class.\n",
    "    # one box per aspect ratio, so for a predictor layer with 3 ratios then 3 boxes\n",
    "    n_boxes = []\n",
    "    scales = scales # for anchor boxes \n",
    "    variances = variances\n",
    "    steps = [None] * n_predictor_layers\n",
    "    offsets = [None] * n_predictor_layers\n",
    "    \n",
    "    for i in aspect_ratios_per_layer:\n",
    "        n_boxes.append(len(i))\n",
    "        \n",
    "    alpha=1.0\n",
    "    depth_multiplier=1\n",
    "    dropout=1e-3\n",
    "    \n",
    "    img_height = input_shape[0]\n",
    "    img_width = input_shape[1]\n",
    "        \n",
    "    x = Input(shape=image_shape)\n",
    "    x1 = Lambda(lambda x: K.tf.image.resize_images(x, (input_shape[0], input_shape[1])))(x)\n",
    "    x1 = Lambda(lambda x: x - np.array(127.5), output_shape=(128, 128, 3),\n",
    "                   name='input_mean_norm')(x)\n",
    "    x1 = Lambda(lambda x: x/np.array(127.5), output_shape=(128, 128, 3),\n",
    "                   name='input_scaler')(x1)\n",
    "    \n",
    "    conv = _conv_block(x1, 32, alpha, strides=(2, 2))\n",
    "    d1 = _depthwise_conv_block(conv, 64, alpha, depth_multiplier, block_id=1)\n",
    "\n",
    "    d2 = _depthwise_conv_block(d1, 128, alpha, depth_multiplier,\n",
    "                              strides=(2, 2), block_id=2)\n",
    "    d3 = _depthwise_conv_block(d2, 128, alpha, depth_multiplier, block_id=3)\n",
    "\n",
    "    d4 = _depthwise_conv_block(d3, 256, alpha, depth_multiplier,\n",
    "                              strides=(2, 2), block_id=4)\n",
    "    d5 = _depthwise_conv_block(d4, 256, alpha, depth_multiplier, block_id=5)\n",
    "\n",
    "    d6 = _depthwise_conv_block(d5, 512, alpha, depth_multiplier,\n",
    "                              strides=(2, 2), block_id=6)\n",
    "    d7 = _depthwise_conv_block(d6, 512, alpha, depth_multiplier, block_id=7)\n",
    "    d8 = _depthwise_conv_block(d7, 512, alpha, depth_multiplier, block_id=8)\n",
    "    d9 = _depthwise_conv_block(d8, 512, alpha, depth_multiplier, block_id=9)\n",
    "    d10 = _depthwise_conv_block(d9, 512, alpha, depth_multiplier, block_id=10)\n",
    "    d11 = _depthwise_conv_block(d10, 512, alpha, depth_multiplier, block_id=11)\n",
    "\n",
    "    d12 = _depthwise_conv_block(d11, 1024, alpha, depth_multiplier,\n",
    "                              strides=(2, 2), block_id=12)\n",
    "    d13 = _depthwise_conv_block(d12, 1024, alpha, depth_multiplier, block_id=13)\n",
    "    \n",
    "    \n",
    "    classes4 = Conv2D(n_boxes[0] * n_classes, (3, 3), strides=(1, 1), \n",
    "                      padding=\"same\", kernel_initializer='glorot_normal', \n",
    "                      kernel_regularizer=l2(l2_reg), name='classes4')(d10)\n",
    "    classes5 = Conv2D(n_boxes[1] * n_classes, (3, 3), strides=(1, 1), \n",
    "                      padding=\"same\", kernel_initializer='glorot_normal', \n",
    "                      kernel_regularizer=l2(l2_reg), name='classes5')(d11)\n",
    "    classes6 = Conv2D(n_boxes[2] * n_classes, (3, 3), strides=(1, 1), \n",
    "                      padding=\"same\", kernel_initializer='glorot_normal', \n",
    "                      kernel_regularizer=l2(l2_reg), name='classes6')(d12)\n",
    "    classes7 = Conv2D(n_boxes[3] * n_classes, (3, 3), strides=(1, 1), padding=\"same\", \n",
    "                      kernel_initializer='glorot_normal', kernel_regularizer=l2(l2_reg), \n",
    "                      name='classes7')(d13)\n",
    "    \n",
    "    \n",
    "    # 4 coords per box -- xmin, xmax, ymun, ymax\n",
    "    box4 = Conv2D(n_boxes[0] * 4, (3, 3), strides=(1, 1), padding=\"same\", \n",
    "                  kernel_initializer='glorot_normal', kernel_regularizer=l2(l2_reg), \n",
    "                  name='box4')(d10)\n",
    "    box5 = Conv2D(n_boxes[1] * 4, (3, 3), strides=(1, 1), padding=\"same\", \n",
    "                  kernel_initializer='glorot_normal', kernel_regularizer=l2(l2_reg), \n",
    "                  name='box5')(d11)\n",
    "    box6 = Conv2D(n_boxes[2] * 4, (3, 3), strides=(1, 1), padding=\"same\", \n",
    "                  kernel_initializer='glorot_normal', kernel_regularizer=l2(l2_reg), \n",
    "                  name='box6')(d12)\n",
    "    box7 = Conv2D(n_boxes[3] * 4, (3, 3), strides=(1, 1), padding=\"same\", \n",
    "                  kernel_initializer='glorot_normal', kernel_regularizer=l2(l2_reg), \n",
    "                  name='box7')(d13)\n",
    "    \n",
    "    \n",
    "    #use the custom anchorbox layer to generate predictions for boxes\n",
    "    anchors4 = AnchorBoxes(img_height, img_width, this_scale=scales[0], next_scale=scales[1], aspect_ratios=aspect_ratios_per_layer[0],\n",
    "                           two_boxes_for_ar1=False, this_steps=steps[0], this_offsets=offsets[0],\n",
    "                           limit_boxes=False, variances=variances, coords='centroids', \n",
    "                           normalize_coords=False, name='anchors4')(box4)\n",
    "    anchors5 = AnchorBoxes(img_height, img_width, this_scale=scales[1], next_scale=scales[2], aspect_ratios=aspect_ratios_per_layer[1],\n",
    "                           two_boxes_for_ar1=False, this_steps=steps[1], this_offsets=offsets[1],\n",
    "                           limit_boxes=False, variances=variances, coords='centroids', normalize_coords=False, name='anchors5')(box5)\n",
    "    anchors6 = AnchorBoxes(img_height, img_width, this_scale=scales[2], next_scale=scales[3], aspect_ratios=aspect_ratios_per_layer[2],\n",
    "                           two_boxes_for_ar1=False, this_steps=steps[2], this_offsets=offsets[2],\n",
    "                           limit_boxes=False, variances=variances, coords='centroids', normalize_coords=False, name='anchors6')(box6)\n",
    "    anchors7 = AnchorBoxes(img_height, img_width, this_scale=scales[3], next_scale=scales[4], aspect_ratios=aspect_ratios_per_layer[3],\n",
    "                           two_boxes_for_ar1=False, this_steps=steps[3], this_offsets=offsets[3],\n",
    "                           limit_boxes=False, variances=variances, coords='centroids', normalize_coords=False, name='anchors7')(box7)\n",
    "    \n",
    "    \n",
    "    # Reshape the class predictors tensor into (batch, height * width * n_boxes, n_classes) => the class is now the last variable for predcition\n",
    "    classes4_reshaped = Reshape((-1, n_classes), name='classes4_reshape')(classes4)\n",
    "    classes5_reshaped = Reshape((-1, n_classes), name='classes5_reshape')(classes5)\n",
    "    classes6_reshaped = Reshape((-1, n_classes), name='classes6_reshape')(classes6)\n",
    "    classes7_reshaped = Reshape((-1, n_classes), name='classes7_reshape')(classes7)\n",
    "    \n",
    "    # Reshape the box coords predictions, into tensors of shape (batch, height * width * n_boxes, 4) => box positions last for loss\n",
    "    # We want the four box coordinates isolated in the last axis to compute the smooth L1 loss\n",
    "    boxes4_reshaped = Reshape((-1, 4), name='boxes4_reshape')(box4)\n",
    "    boxes5_reshaped = Reshape((-1, 4), name='boxes5_reshape')(box5)\n",
    "    boxes6_reshaped = Reshape((-1, 4), name='boxes6_reshape')(box6)\n",
    "    boxes7_reshaped = Reshape((-1, 4), name='boxes7_reshape')(box7)\n",
    "    # Reshape the anchor box tensors, into tensors of shape (batch, height * width * n_boxes, 8)\n",
    "    anchors4_reshaped = Reshape((-1, 8), name='anchors4_reshape')(anchors4)\n",
    "    anchors5_reshaped = Reshape((-1, 8), name='anchors5_reshape')(anchors5)\n",
    "    anchors6_reshaped = Reshape((-1, 8), name='anchors6_reshape')(anchors6)\n",
    "    anchors7_reshaped = Reshape((-1, 8), name='anchors7_reshape')(anchors7)\n",
    "    #concat along the middle axis, since want to keep the batch and n_classes/8 untouched\n",
    "    class_concat = Concatenate(axis=1, name='concatenate_classes')([classes4_reshaped, \n",
    "                                classes5_reshaped,classes6_reshaped, \n",
    "                                classes7_reshaped])\n",
    "    boxes_concat = Concatenate(axis=1, name='concatenate_boxes')([boxes4_reshaped,\n",
    "                                                             boxes5_reshaped,\n",
    "                                                             boxes6_reshaped,\n",
    "                                                             boxes7_reshaped])\n",
    "\n",
    "    # Output shape of `anchors_final`: (batch, n_boxes_total, 8)\n",
    "    anchors_concat = Concatenate(axis=1, name='concatenate_anchors')([anchors4_reshaped,\n",
    "                                                                 anchors5_reshaped,\n",
    "                                                                 anchors6_reshaped,\n",
    "                                                                 anchors7_reshaped])\n",
    "    \n",
    "    classification_softmax = Activation('softmax', name='classes_softamx')(class_concat)\n",
    "    \n",
    "    prediction = Concatenate(axis=2,name='concatenate_output')([classification_softmax,\n",
    "                                                              boxes_concat,\n",
    "                                                              anchors_concat ])\n",
    "    \n",
    "    model = Model(inputs=x, outputs=prediction)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.clear_session() # Clear previous models from memory.\n",
    "\n",
    "model = mobilenet_model((300, 480, 3), 5, l2_reg=0.005, \n",
    "                aspect_ratios_per_layer=[[1.0, 2.0, 0.5],\n",
    "                                         [1.0, 2.0, 0.5],\n",
    "                                         [1.0, 2.0, 0.5],\n",
    "                                         [1.0, 2.0, 0.5]])\n",
    "\n",
    "# 3: Instantiate an Adam optimizer and the SSD loss function and compile the model\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=5e-04)\n",
    "\n",
    "# with negative hard mining as per paper\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, n_neg_min=0, alpha=1.0)\n",
    "\n",
    "model.compile(optimizer=adam, loss=ssd_loss.compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
