{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import tensorflow as tf\n",
    "\n",
    "class FCN:\n",
    "    def __init__(self, batch_size, epochs, lr, regularization_const, saveDirectory, vgg_path):\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.regularization_const = regularization_const\n",
    "        self.saveDirectory = saveDirectory\n",
    "        self.vgg_path = num_layers\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.global_step = None\n",
    "        \n",
    "\n",
    "        self.features_placeholder = None\n",
    "        self.features_lengths_placeholder = None\n",
    "        self.labels_placeholder = None\n",
    "        self.prediction = None\n",
    "        self.prediction_class = None\n",
    "        self.prediction_softmax = None\n",
    "        self.train_op = None\n",
    "\n",
    "        self.loss = None\n",
    "        self.accuracy = None\n",
    "\n",
    "        self.saveDirectory = \"checkpoint/\", self.name)\n",
    "    \n",
    "    \n",
    "    def load_model(self, sess):\n",
    "        print(\"Will restore model from {path}\".format(path=self.saveDirectory))\n",
    "        training_saver = tf.train.Saver(max_to_keep=None)\n",
    "        training_saver.restore(sess, self.saveDirectory)\n",
    "\n",
    "    def save_model(self, sess, epoch):\n",
    "        logging.info(\"Will save model at epoch {epoch} to {path}\".format(epoch=epoch, path=self.saveDirectory))\n",
    "        training_saver = tf.train.Saver(max_to_keep=None)\n",
    "        training_saver.save(sess, self.saveDirectory, global_step=epoch)\n",
    "\n",
    "    def train_model(self, sess, other_operations=list(), keep_prob=.5):\n",
    "        operations_to_run = [self.train_op] + other_operations\n",
    "\n",
    "        return sess.run(operations_to_run, feed_dict={\n",
    "            self.keep_prob: keep_prob\n",
    "        })\n",
    "    \n",
    "    def load_vgg(self, sess, vgg_path):\n",
    "        \"\"\"\n",
    "        Load Pretrained VGG Model into TensorFlow.\n",
    "        :param sess: TensorFlow Session\n",
    "        :param vgg_path: Path to vgg folder, containing \"variables/\" and \"saved_model.pb\"\n",
    "        :return: Tuple of Tensors from VGG model (image_input, keep_prob, layer3_out, layer4_out, layer7_out)\n",
    "        \"\"\"\n",
    "\n",
    "        vgg_tag = 'vgg16'\n",
    "        vgg_input_tensor_name = 'image_input:0'\n",
    "        vgg_keep_prob_tensor_name = 'keep_prob:0'\n",
    "        vgg_layer3_out_tensor_name = 'layer3_out:0'\n",
    "        vgg_layer4_out_tensor_name = 'layer4_out:0'\n",
    "        vgg_layer7_out_tensor_name = 'layer7_out:0'\n",
    "        model = tf.saved_model.loader.load(\n",
    "            sess,\n",
    "            [vgg_tag],\n",
    "            vgg_path\n",
    "        )\n",
    "        t1 = tf.get_default_graph().get_tensor_by_name(vgg_input_tensor_name)\n",
    "        t2 = tf.get_default_graph().get_tensor_by_name(vgg_keep_prob_tensor_name)\n",
    "        t3 = tf.get_default_graph().get_tensor_by_name(vgg_layer3_out_tensor_name)\n",
    "        t4 = tf.get_default_graph().get_tensor_by_name(vgg_layer4_out_tensor_name)\n",
    "        t5 = tf.get_default_graph().get_tensor_by_name(vgg_layer7_out_tensor_name)\n",
    "\n",
    "        tf.summary.image('image', t1)\n",
    "        tf.summary.histogram('vgg3', t3)\n",
    "        tf.summary.histogram('vgg4', t4)\n",
    "        tf.summary.histogram('vgg7', t5)\n",
    "\n",
    "        return t1, t2,t3,t4,t5\n",
    "        \n",
    "class LSTM:\n",
    "    def __init__(self,\n",
    "                 batch_size,\n",
    "                 vocabulary_size,\n",
    "                 lstm_units,\n",
    "                 max_input_series_length,\n",
    "                 num_layers,\n",
    "                 output_classes_count,\n",
    "                 lr\n",
    "                 ):\n",
    "        self.batch_size = batch_size\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.lstm_units = lstm_units\n",
    "        self.max_input_series_length = max_input_series_length\n",
    "        self.num_layers = num_layers\n",
    "        self.output_classes_count = output_classes_count\n",
    "        self.lr = lr\n",
    "\n",
    "        self.name = \"LSTM_batch{batch}_vocab{vocab}_strpPunct{strip_punctuation}_lemm{lemmatize}_strpStopwords{strip_stopwords}\" \\\n",
    "                    \"_fwPadd{forward_padding}_maxLen{max_doc_length}_testSet{test_split_ratio}\" \\\n",
    "                    \"_nHidden{n_hidden_layers}_lstmUnits{lstm_units}_keepProb{keep_prob}_epochs{epochs}_lr{learning_rate}\".format(\n",
    "            batch=self.batch_size,\n",
    "            vocab=self.vocabulary_size,\n",
    "            max_doc_length=self.max_input_series_length,\n",
    "            n_hidden_layers=self.num_layers,\n",
    "            lstm_units=self.lstm_units,\n",
    "            strip_punctuation=FLAGS.strip_punctuation,\n",
    "            lemmatize=FLAGS.lemmatize,\n",
    "            strip_stopwords=FLAGS.strip_stopwords,\n",
    "            forward_padding=FLAGS.forward_padding,\n",
    "            test_split_ratio=FLAGS.test_split_ratio,\n",
    "            keep_prob=FLAGS.keep_prob,\n",
    "            epochs=FLAGS.epochs,\n",
    "            learning_rate=FLAGS.lr\n",
    "        )\n",
    "\n",
    "\n",
    "        self.global_step = None\n",
    "        self.keep_prob = None\n",
    "\n",
    "\n",
    "        self.features_placeholder = None\n",
    "        self.features_lengths_placeholder = None\n",
    "        self.labels_placeholder = None\n",
    "        self.prediction = None\n",
    "        self.prediction_class = None\n",
    "        self.prediction_softmax = None\n",
    "        self.train_op = None\n",
    "\n",
    "        self.loss = None\n",
    "        self.accuracy = None\n",
    "\n",
    "        self.saveDirectory = join(getattr(FLAGS, 'job-dir'), \"checkpoint\", self.name)\n",
    "\n",
    "    def build_model(self, features, features_length, labels):\n",
    "        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "        embedding_size = self.lstm_units\n",
    "\n",
    "        with tf.name_scope(\"input_data\"):\n",
    "            self.features_placeholder = features\n",
    "            self.features_lengths_placeholder = features_length\n",
    "            self.labels_placeholder = labels\n",
    "\n",
    "\n",
    "            # self.features_placeholder = tf.placeholder(\n",
    "            #     tf.int32,\n",
    "            #     shape=[None, self.max_input_series_length],\n",
    "            #     name=\"features_placeholder\"\n",
    "            # )\n",
    "            # self.features_lengths_placeholder = tf.placeholder(\n",
    "            #     tf.int32,\n",
    "            #     shape=[None],\n",
    "            #     name=\"features_lengths_placeholder\"\n",
    "            # )\n",
    "            # self.labels_placeholder = tf.placeholder(tf.int32, shape=[None], name=\"labels_placeholder\")\n",
    "\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            word_embeddings = tf.get_variable(\"word_embeddings\", [self.vocabulary_size, embedding_size])\n",
    "            embedded_vecs = tf.nn.embedding_lookup(word_embeddings, self.features_placeholder)\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"hidden\"):\n",
    "            self.keep_prob = tf.placeholder_with_default(1.0, shape=())\n",
    "            lstm_cell = tf.contrib.rnn.BasicLSTMCell(self.lstm_units)\n",
    "            lstm_cell = tf.contrib.rnn.DropoutWrapper(cell=lstm_cell, output_keep_prob=self.keep_prob)\n",
    "\n",
    "            cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * self.num_layers)\n",
    "            rnn_outputs, rnn_final_state = tf.nn.dynamic_rnn(\n",
    "                cell,\n",
    "                embedded_vecs,\n",
    "                dtype=tf.float32,\n",
    "                sequence_length=self.features_lengths_placeholder\n",
    "            )\n",
    "\n",
    "        with tf.name_scope(\"output_layer\"):\n",
    "            weight = tf.Variable(tf.truncated_normal([self.lstm_units, self.output_classes_count]), name=\"weight\")\n",
    "            bias = tf.Variable(tf.constant(0.1, shape=[self.output_classes_count]), name=\"bias\")\n",
    "            rnn_outputs = tf.transpose(rnn_outputs, [1, 0, 2])\n",
    "            # last = tf.gather(rnn_outputs, int(rnn_outputs.get_shape()[0]) - 1)\n",
    "\n",
    "            last = rnn_final_state[-1].h\n",
    "            self.prediction = (tf.matmul(last, weight) + bias)\n",
    "            self.prediction_class = tf.argmax(self.prediction, -1, output_type=tf.int32, name=\"prediction_class\")\n",
    "\n",
    "            self.prediction_softmax = tf.nn.softmax(self.prediction)\n",
    "\n",
    "        with tf.name_scope(\"metrics\"):\n",
    "            correct_pred = tf.equal(self.prediction_class, self.labels_placeholder)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "            self.loss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    logits=self.prediction,\n",
    "                    labels=tf.one_hot(self.labels_placeholder, self.output_classes_count)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        tf.summary.scalar('Loss', self.loss)\n",
    "        tf.summary.scalar('Accuracy', self.accuracy)\n",
    "\n",
    "        self.train_op = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "    def load_model(self, sess):\n",
    "        logging.info(\"Will restore model from {path}\".format(path=self.saveDirectory))\n",
    "        training_saver = tf.train.Saver(max_to_keep=None)\n",
    "        training_saver.restore(sess, self.saveDirectory)\n",
    "\n",
    "    def save_model(self, sess, epoch):\n",
    "        logging.info(\"Will save model at epoch {epoch} to {path}\".format(epoch=epoch, path=self.saveDirectory))\n",
    "        training_saver = tf.train.Saver(max_to_keep=None)\n",
    "        training_saver.save(sess, self.saveDirectory, global_step=epoch)\n",
    "\n",
    "    def train_model(self, sess, other_operations=list(), keep_prob=.5):\n",
    "        operations_to_run = [self.train_op] + other_operations\n",
    "\n",
    "        return sess.run(operations_to_run, feed_dict={\n",
    "            self.keep_prob: keep_prob\n",
    "        })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
